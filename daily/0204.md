# 强化学习入门

### 如何解释强化和学习？

学习就是训练，强化就是行为策略的加强

### 强化学习的起源

最早是RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）之所以带上HF是因为其对齐目标是人类价值观，人类价值观是很难通过一个特定规则的监督模型来给出奖惩值，因此通过人来标注反馈数据去训练一个奖励模型，但随着LLM发展，openai发布了o1和o3模型，其中下游任务中直接使用强化学习进行训练，也叫做RFT或者RLFT（强化学习微调），之所以不需要HF（human feedback）是因为在代码或者逻辑数学运算，再或者其他下游任务下天然是存在奖励模型的，如生成的代码是否可以运行，数学运算结果是否正确，Agent任务是否执行成功等等，在这些任务上是不需要人类反馈去训练奖励模型的。

### 一些专有名词概念

[On-Policy](https://zhida.zhihu.com/search?content_id=256921237&content_type=Article&match_order=1&q=On-Policy&zhida_source=entity): 训练数据由需要训练的策略本身通过与环境的互动产生，用自己产生的数据来进行训练（可以理解为需要实时互动）

[Off-Policy](https://zhida.zhihu.com/search?content_id=256921237&content_type=Article&match_order=1&q=Off-Policy&zhida_source=entity): 训练数据预先收集好（人工或者其它策略产生），策略直接通过这些数据进行学习。

### 一些强化学习的方式

#### PPO（Policy gradient optimization，策略梯度优化）
