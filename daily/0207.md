1、论文阅读《**美团LongCat**》，加了一个zero-expert，当被分类到这里的时候，直接采用输入为输出，不置为0向量，也不用专家模型

问：美团的longcat，加了一个zero-expert，意思就是用了输入的原始向量，那为什么普通的moe采用的是0向量，而不能用输入的原始向量呢

答：普通 MoE 用 0 向量，本质是：

> “这个 expert 不贡献信息。”

LongCat 的 zero-expert 用原始输入，本质是：

> “跳过 FFN，让信息直接沿 residual 传递。”

transformer的ffn本质是：f(x) = x  + ffn(x)

 如果是不涉及的专家，也只是置零，矩阵也需要进行计算，ffn出来是0的话，那就代表着一层跳过了ffn，只做了一层残差转换

但是如果和美团的longcat一样的话，就会变成f(x) = x + x，x就变成了double特征，会放大激活值，所以在普通moe里，置0比较安全

但是在longcat里，他是直接把FFN都跳过了，根本不会进去计算

所以其实longcat的优势在于：简单token不需要进行ffn计算，Zero-expert token 不更新 FFN 参数 → 参数更新更专一，以下是gpt总结：

**LongCat 优势 = 真实 token-adaptive compute + sparse scaling + 减少梯度干扰**

- 普通 MoE只是**稀疏专家 + 输出0**，算力节省有限
- LongCat 是**真正跳过计算 + 动态算力分配 + token难度感知**，所以训练大模型更高效，性能更强。

2、今天搞懂了moe-ffn-transformer的关系，我好聪明

3、qwen3模型架构明天看